{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark >> None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8F8_NNT54Zf",
        "outputId": "fd20be43-ad53-4080-e974-69494f06aa71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=8be19a1617457555d49d7360bb147b3ad33f43fd0c2c32e3b3b955b5c6e4dcfc\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "asJ5h-5q8LA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1\n",
        "Вывод данных в консоль\n"
      ],
      "metadata": {
        "id": "hJONvAix5zzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем \"CountElements\".\n",
        "2. Читаем поток данных с использованием источника \"rate\", который автоматически генерирует данные. Этот источник используется для тестирования и отладки, поскольку он позволяет легко создавать потоки данных без необходимости подключения к внешним системам.\n",
        "3. Записываем полученные данные в консоль с использованием режима вывода \"append\". В этом режиме в консоль будут выводиться только новые строки, полученные в каждом микро-пакете данных.\n",
        "4. Запускаем поток обработки данных и ожидает его завершения.\n"
      ],
      "metadata": {
        "id": "yTvqNZo758jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CountElements\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "5K_V_6Kf3-nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2:\n",
        "Фильтрация чисел (четные)\n"
      ],
      "metadata": {
        "id": "X6yBx_7h7GeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"FilterEvenNumbers\". Это необходимо для инициализации Spark и подготовки к выполнению операций с данными.\n",
        "\n",
        "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и демонстрации обработки потоковых данных.\n",
        "\n",
        "3. Фильтруем полученные данные, оставляя только те строки, где значение поля \"value\" делится на 2 без остатка, то есть числа, являющиеся четными.\n",
        "\n",
        "4. Записываем отфильтрованные данные обратно в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоке, новые данные будут добавлены к уже существующим, не удаляя предыдущие записи.\n",
        "\n",
        "5. Запускаем потоковую запись и ожидает завершения работы запроса в течение 5 секунд. Это позволяет увидеть результаты обработки данных в консоли в течение этого времени.\n"
      ],
      "metadata": {
        "id": "3q_5imWc4i7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"FilterEvenNumbers\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_even = df.filter(\"value % 2 == 0\")\n",
        "query = df_even.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrT_u-Te6O_u",
        "outputId": "8069bc9d-cded-47bc-a006-929f87846386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3:\n",
        "Сгруппировать по значениям и посчитать количество\n"
      ],
      "metadata": {
        "id": "YtgLHHmE7f9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"GroupByValue\". Это необходимо для работы с Spark и его функциональностью, включая потоковую обработку данных.\n",
        "\n",
        "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с постоянной скоростью, что удобно для тестирования и демонстрации потоковой обработки.\n",
        "\n",
        "3. Группируем полученные данные по столбцу \"value\" и подсчитывает количество записей для каждого уникального значения в этом столбце. Это позволяет агрегировать данные по определенному критерию.\n",
        "\n",
        "4. Запускаем потоковую запись результатов агрегации в консоль с использованием режима вывода \"update\". Режим \"update\" означает, что в консоль будут выводиться только обновленные результаты агрегации, а не полный набор данных при каждом обновлении. Это делает вывод более читаемым и эффективным, особенно когда данные постоянно обновляются.\n",
        "\n",
        "5. Ожидаем завершения потоковой обработки в течение 5 секунд. Это означает, что поток будет работать в течение этого времени, а затем завершится.\n"
      ],
      "metadata": {
        "id": "zCsmYvdT4vzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"GroupByValue\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_grouped = df.groupBy(\"value\").count()\n",
        "query = df_grouped.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNG16a4I7LFA",
        "outputId": "588fdabd-18a8-4d9f-e462-249a6da8e2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4\n",
        "Вычислить сумму значений\n"
      ],
      "metadata": {
        "id": "L-mPJYiE7mKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем экземпляр SparkSession с именем приложения \"SumValues\". Этот шаг необходим для инициализации Spark и подготовки среды для выполнения операций с данными.\n",
        "\n",
        "2. Читаем поток данных из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки. В данном случае, данные будут генерироваться автоматически без необходимости подключения к внешним источникам данных.\n",
        "\n",
        "3. Выполняем агрегацию данных, вычисляя сумму значений в столбце \"value\" и сохраняя результат в новом столбце \"total\". Это делается с помощью метода selectExpr, который позволяет выполнять SQL-подобные выражения для трансформации данных.\n",
        "\n",
        "4. Записываем результаты агрегации в консоль с использованием формата \"console\". Это позволяет наблюдать за изменениями в реальном времени, так как данные будут выводиться в консоль.\n",
        "\n",
        "5. Запускаем запрос на потоковую обработку данных с использованием режима вывода \"update\". В этом режиме, при каждом обновлении данных, будет выводиться только текущее состояние агрегации, что позволяет видеть актуальную сумму значений.\n",
        "\n",
        "6. Ожидаем завершения обработки потока данных в течение 5 секунд с помощью метода awaitTermination. Это означает, что приложение будет работать в течение этого времени, обрабатывая поступающие данные и выводя результаты в консоль.\n"
      ],
      "metadata": {
        "id": "fBpp4yFv48Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"SumValues\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_sum = df.selectExpr(\"sum(value) AS total\")\n",
        "query = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xiJZWI6f7lzl",
        "outputId": "b13d34ef-8c34-4a6e-a242-77611147035a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SparkSession' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f6543506e4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SumValues\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum(value) AS total\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 5\n",
        "Найти максимальное значение"
      ],
      "metadata": {
        "id": "vSk3qT1I7vCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"MaxValue\" с помощью SparkSession.builder.appName(\"MaxValue\").getOrCreate(). Это необходимо для работы с Spark и его функциональностью, включая структурированный потоковый анализ.\n",
        "\n",
        "2. Читаем поток данных с использованием источника \"rate\". Источник \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и разработки. В данном случае, данные читаются без дополнительных параметров, что означает использование значений по умолчанию для генерации данных.\n",
        "\n",
        "3. Выполняем агрегацию данных, вычисляя максимальное значение поля 'value' в каждом микро-батче данных. Это достигается с помощью метода agg(f.max('value')), где f - это ссылка на модуль pyspark.sql.functions, который предоставляет функции для работы с данными.\n",
        "\n",
        "4. Записываем результаты агрегации в консоль с использованием writeStream.outputMode(\"update\").format(\"console\").start(). Здесь outputMode(\"update\") указывает, что при каждом обновлении данных в потоке, результаты агрегации будут выводиться в консоль. format(\"console\") указывает, что вывод должен быть направлен в консоль, а не в файл или базу данных.\n",
        "\n",
        "5. Ожидаем завершения потокового запроса с помощью query.awaitTermination(5), что означает, что поток будет работать в течение 5 секунд, после чего будет выполнено завершение работы.\n"
      ],
      "metadata": {
        "id": "U4YRbmkS5J4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"MaxValue\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_max = df.agg(f.max('value'))\n",
        "query = df_max.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "id": "DjPrxvmF5QKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 6\n",
        "Вычислить скользящее окно по значению\n"
      ],
      "metadata": {
        "id": "nkgaFpi472pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем \"SlidingWindow\".\n",
        "2. Читаем потоковые данные из источника \"rate\", который генерирует данные с постоянной скоростью.\n",
        "3. Группируем эти данные по временным окнам, размером в 10 минут, и подсчитывает количество записей в каждом окне.\n",
        "4. Записываем результаты обработки в консоль в режиме обновления (outputMode(\"update\")), что означает, что в консоль будут выводиться только обновленные агрегированные результаты.\n",
        "5. Запускаем потоковую запрос и ожидает его завершения в течение 5 секунд.\n"
      ],
      "metadata": {
        "id": "G2wCQyBC5aD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"SlidingWindow\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_windowed = df.groupBy(window(\"timestamp\", \"10 minutes\")).count()\n",
        "query = df_windowed.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "id": "qgGCMPhD5Rnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 7\n",
        "Соединение потоков данных"
      ],
      "metadata": {
        "id": "F9Tm_dbj79n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"JoinStreams\".\n",
        "2. Читаем два потока данных, используя формат \"rate\", который генерирует данные с определенной скоростью. По умолчанию, каждый поток будет генерировать одну строку в секунду с полями timestamp и value.\n",
        "3. Соединяем два потока данных по полю value. Это означает, что для каждой пары строк из двух потоков, где значение поля value совпадает, будет создана новая строка в результате соединения.\n",
        "4. Записываем результат соединения в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоках, новые строки будут добавляться к выводу, не удаляя предыдущие строки.\n",
        "5. Запускаем запрос на потоковую обработку и ожидает его завершения в течение 5 секунд.\n"
      ],
      "metadata": {
        "id": "013d5A_d5luj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"JoinStreams\").getOrCreate()\n",
        "df1 = spark.readStream.format(\"rate\").load()\n",
        "df2 = spark.readStream.format(\"rate\").load()\n",
        "df_joined = df1.join(df2, \"value\")\n",
        "query = df_joined.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "id": "EhpFy8Ut76yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 8\n",
        "Запись данных в файл\n"
      ],
      "metadata": {
        "id": "Wzea5l1S8C9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем экземпляр SparkSession с именем приложения \"WriteToFile\". SparkSession является точкой входа для работы с Spark и предоставляет API для работы с DataFrame и DataSet.\n",
        "\n",
        "2. Читаем потоковые данные с использованием формата \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки потоковых приложений. В данном случае, данные будут генерироваться без дополнительных параметров, что означает использование значений по умолчанию.\n",
        "\n",
        "3. Записываем полученные потоковые данные в формате Parquet в указанный каталог \"output/\". Parquet - это эффективный формат хранения данных, который обеспечивает высокую производительность и эффективное сжатие.\n",
        "\n",
        "4. Запускаем потоковую запись данных и ожидает завершения работы запроса в течение 5 секунд. Метод awaitTermination блокирует выполнение программы до тех пор, пока потоковая запись не будет остановлена или не произойдет ошибка.\n"
      ],
      "metadata": {
        "id": "me1-Pd1e5vE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"WriteToFile\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "query = df.writeStream.format(\"parquet\").option(\"path\", \"output/\").start()\n",
        "query.awaitTermination(5)"
      ],
      "metadata": {
        "id": "EqhFZzIB8DvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}