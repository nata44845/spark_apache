{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GV_vyWGDpeJ7"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgFprDfupXpy"
      },
      "source": [
        "### Задание 1\n",
        "Дан набор данных с информацией о студентах и их оценках по предметам. Необходимо использовать функцию pivot в pyspark для трансформации данных таким образом, чтобы получить таблицу, в которой строки будут соответствовать студентам, столбцы — предметам, а значения — их оценкам. Затем вывести средние оценки студентов по каждому предмету."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IactQHNprmV"
      },
      "source": [
        "1. Инициализируем сессию Spark с именем приложения \"pivot_example\".\n",
        "2. Загружаем исходные данные в DataFrame df, который содержит имена студентов, предметы и оценки.\n",
        "3. Применяем функцию pivot к DataFrame df, группируя данные по столбцу \"name\" и создавая новые столбцы на основе уникальных значений в столбце \"subject\". Для каждого уникального значения в столбце \"subject\" создается новый столбец.\n",
        "4. Заполняем новые столбцы средними значениями оценок из столбца \"grade\" для каждой комбинации \"name\" и \"subject\".\n",
        "5. Выводим результат на экран, показывая средние оценки студентов по каждому предмету.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQz9ELNin3MG",
        "outputId": "3d7c8997-6c3b-48b2-bf38-ddd2f563af98"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "[CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.network.timeout\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_example\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.network.timeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m800s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Увеличение общего таймаута\u001b[39;00m\n\u001b[0;32m      8\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.heartbeatInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m60s\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Увеличение интервала пульсации\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Загрузка исходных данных\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Nata\\GeekBrains\\gb-git\\spark_apache\\venv_spark_apache\\Lib\\site-packages\\pyspark\\sql\\conf.py:43\u001b[0m, in \u001b[0;36mRuntimeConfig.set\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mbool\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Nata\\GeekBrains\\gb-git\\spark_apache\\venv_spark_apache\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Nata\\GeekBrains\\gb-git\\spark_apache\\venv_spark_apache\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[1;31mAnalysisException\u001b[0m: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.network.timeout\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"pivot_example\").getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.network.timeout\", \"800s\")  # Увеличение общего таймаута\n",
        "spark.conf.set(\"spark.executor.heartbeatInterval\", \"60s\")  # Увеличение интервала пульсации\n",
        "\n",
        "# Загрузка исходных данных\n",
        "data = [(\"Alice\", \"Math\", 90),\n",
        "    \t(\"Alice\", \"Physics\", 85),\n",
        "        (\"Bob\", \"Math\", 70),\n",
        "        (\"Bob\", \"Physics\", 75),\n",
        "        (\"Bob\", \"History\", 80),\n",
        "        (\"John\", \"Math\", 65),\n",
        "        (\"John\", \"Physics\", 75),\n",
        "        (\"Jane\", \"Math\", 100),\n",
        "    \t(\"Jane\", \"Physics\", 90),\n",
        "        (\"Jane\", \"History\", 95)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"subject\", \"grade\"])\n",
        "\n",
        "# Применение функции pivot\n",
        "pivot_df = df.groupBy(\"name\").pivot(\"subject\").agg(F.avg(\"grade\"))\n",
        "\n",
        "# Вывод средних оценок студентов по каждому предмету\n",
        "pivot_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QQH4LffVq3jQ"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ArwRM2xp8n_"
      },
      "source": [
        "### Задание 2\n",
        "Найти среднее значение продаж для каждого месяца по каждому продукту, а также для каждого продукта найти месяц с наибольшим объемом продаж.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io0ELPmHqPkB"
      },
      "source": [
        "1. Инициализируем SparkSession: Создаем сессия Spark, которая является точкой входа для работы с Spark API. Это необходимо для выполнения операций с данными.\n",
        "\n",
        "2. Загружаем данных о продажах: Создаем список кортежей, представляющих данные о продажах, и на основе этого списка создается DataFrame sales_df. Каждый кортеж содержит информацию о продаже: уникальный идентификатор (id), дату продажи (date), название продукта (product) и сумму продажи (amount).\n",
        "\n",
        "3. Определяем окна для вычисления среднего значения продаж по месяцам: Создаем окно monthly_avg_sales_window, которое разделяет данные на группы по продукту и месяцу продажи, а затем упорядочивает данные внутри каждой группы по дате. Это окно используется для вычисления среднего значения продаж по месяцам для каждого продукта.\n",
        "\n",
        "4. Добавляем столбцы со средними значениями продаж по месяцам: В исходный DataFrame sales_df добавляем новый столбец monthly_avg_sales, который содержит среднее значение продаж по месяцам для каждого продукта. Это достигается с помощью оконной функции avg и ранее определенного окна monthly_avg_sales_window.\n",
        "\n",
        "5. Определяем окна для нахождения месяца с наибольшим объемом продаж: Создаем окно max_sales_month_window, которое разделяет данные на группы по продукту и упорядочивает данные внутри каждой группы по убыванию суммы продажи. Это окно используется для вычисления месяца с наибольшим объемом продаж для каждого продукта.\n",
        "\n",
        "6. Добавляем столбца с месяцем, в котором был наибольший объем продаж: В исходный DataFrame sales_df добавляется новый столбец max_sales_month, который содержит номер месяца, в котором был наибольший объем продаж для каждого продукта. Это достигается с помощью оконной функции first и ранее определенного окна max_sales_month_window.\n",
        "\n",
        "7. Выводим результатов: В конце выполнения кода выводится содержимое DataFrame sales_df, включая все добавленные столбцы, что позволяет увидеть результаты вычислений средних значений продаж по месяцам и месяцы с наибольшим объемом продаж для каждого продукта.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRof5ClIqCuY",
        "outputId": "d143ca10-739f-4119-8ad0-33747bfac27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+--------+------+-----------------+---------------+\n",
            "| id|      date| product|amount|monthly_avg_sales|max_sales_month|\n",
            "+---+----------+--------+------+-----------------+---------------+\n",
            "|  3|2021-02-10|product1|   200|            200.0|              2|\n",
            "|  2|2021-01-15|product1|   150|            125.0|              2|\n",
            "|  1|2021-01-01|product1|   100|            100.0|              2|\n",
            "|  5|2021-02-20|product2|   180|            180.0|              2|\n",
            "|  4|2021-01-05|product2|   120|            120.0|              2|\n",
            "|  8|2021-02-22|product3|   220|            165.0|              2|\n",
            "|  7|2021-02-13|product3|   110|            110.0|              2|\n",
            "|  6|2021-01-08|product3|   100|            100.0|              2|\n",
            "|  9|2021-01-02|product4|   225|            225.0|              1|\n",
            "| 10|2021-02-28|product4|    80|             80.0|              1|\n",
            "+---+----------+--------+------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Создание сессии Spark\n",
        "spark = SparkSession.builder.appName(\"window_functions_example\").getOrCreate()\n",
        "\n",
        "# Загрузка данных о продажах\n",
        "sales_data = [(1, '2021-01-01', 'product1', 100),\n",
        "          \t(2, '2021-01-15', 'product1', 150),\n",
        "              (3, '2021-02-10', 'product1', 200),\n",
        " \t         (4, '2021-01-05', 'product2', 120),\n",
        "              (5, '2021-02-20', 'product2', 180),\n",
        "              (6, '2021-01-08', 'product3', 100),\n",
        "              (7, '2021-02-13', 'product3', 110),\n",
        "              (8, '2021-02-22', 'product3', 220),\n",
        "              (9, '2021-01-02', 'product4', 225),\n",
        "              (10, '2021-02-28', 'product4', 80)\n",
        "]\n",
        "sales_df = spark.createDataFrame(sales_data, [\"id\", \"date\", \"product\", \"amount\"])\n",
        "\n",
        "# Оконная функция для нахождения среднего значения продаж по месяцам\n",
        "monthly_avg_sales_window = Window.partitionBy(\"product\", F.month(\"date\")).orderBy(\"date\")\n",
        "\n",
        "# Добавление столбца со средними значениями продаж по месяцам в исходный DataFrame\n",
        "sales_df = sales_df.withColumn(\"monthly_avg_sales\", F.avg(\"amount\").over(monthly_avg_sales_window))\n",
        "\n",
        "# Оконная функция для нахождения месяца с наибольшим объемом продаж\n",
        "max_sales_month_window = Window.partitionBy(\"product\").orderBy(F.desc(\"amount\"))\n",
        "\n",
        "# Добавление столбца с месяцем, в котором был наибольший объем продаж для каждого продукта\n",
        "sales_df = sales_df.withColumn(\"max_sales_month\", F.first(F.month(\"date\")).over(max_sales_month_window))\n",
        "\n",
        "# Вывод результатов\n",
        "sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ri5c6RNlqwr4"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMpfOTiHqggN"
      },
      "source": [
        "### Задание 3\n",
        "Предположим, у вас есть набор данных с информацией о продажах в разных регионах. Вам нужно использовать оконные функции для вычисления среднего объема продаж по регионам за последние 3 месяца. Среднее значение приведите к формату DoubleType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j1FD_prsoUy"
      },
      "source": [
        "1. Инициализируем Spark сессии: Создаем экземпляр SparkSession, который является точкой входа для работы с Spark API. Это необходимо для выполнения операций с данными.\n",
        "\n",
        "2. Создаем DataFrame: Создаем DataFrame из списка кортежей, где каждый кортеж представляет собой строку данных. В данном случае, каждая строка содержит информацию о продажах (sales) в определенном регионе (region) на определенную дату (date).\n",
        "\n",
        "3. Преобразовываем типа данных для поля date: Используем функция to_date для преобразования строкового представления даты в тип данных DateType, что позволяет выполнять дальнейшие операции с датами.\n",
        "\n",
        "4. Вычисляем разницу в днях между текущей датой и датой 90 дней назад: Для каждой строки DataFrame вычисляем количество дней между текущей датой и датой продаж. Это делается с помощью функции datediff, которая принимает текущую дату и дату из DataFrame.\n",
        "\n",
        "5. Определяем оконную спецификацию: Создаем спецификация окна (window_spec), которая определяет, как будут группироваться данные для оконных функций. В данном случае, данные группируются по региону (region) и сортируются по дате (date). Окно определяется так, чтобы включать все строки от начала данных для каждого региона до 90 дней назад от текущей даты.\n",
        "\n",
        "6. Вычисляем среднего объема продаж по регионам за последние 90 дней: Используем оконная функция avg для вычисления среднего значения продаж (sales) в каждом окне, определенном в предыдущем шаге. Это среднее значение будет вычислено для каждой строки в окне, что позволяет получить среднее значение продаж за последние 90 дней для каждого региона.\n",
        "\n",
        "7. Применяем оконную функции и приведение типа к DataFrame: Вычисленное среднее значение продаж за последние 90 дней добавляется в DataFrame как новый столбец avg_sales_last_90_days. Для этого используется метод withColumn, а результат приведения типа к DoubleType гарантирует, что данные будут храниться в формате числа с плавающей точкой.\n",
        "\n",
        "8. Отображаем результаты: В конце выполнения кода, результаты вычислений отображаем с помощью метода show(), что позволяет визуально проверить результаты работы оконной функции.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mQDHbSMqEjS",
        "outputId": "0a7d373d-07f6-476c-cb1b-b39fc59930bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+----------+-----+-------------+----------------------+\n",
            "| id| region|      date|sales|days_since_90|avg_sales_last_90_days|\n",
            "+---+-------+----------+-----+-------------+----------------------+\n",
            "|  1|Регион1|2022-01-01|100.0|          843|                 125.0|\n",
            "|  2|Регион1|2022-02-15|150.0|          798|                 125.0|\n",
            "|  3|Регион2|2022-01-10|120.0|          834|                 140.0|\n",
            "|  4|Регион2|2022-02-28|200.0|          785|                 140.0|\n",
            "|  5|Регион2|2022-03-31|100.0|          754|                 140.0|\n",
            "|  6|Регион3|2022-01-15|150.0|          829|    156.66666666666666|\n",
            "|  7|Регион3|2022-02-11|120.0|          802|    156.66666666666666|\n",
            "|  8|Регион3|2022-03-28|200.0|          757|    156.66666666666666|\n",
            "|  9|Регион4|2022-01-03|100.0|          841|                 142.5|\n",
            "| 10|Регион4|2022-02-18|150.0|          795|                 142.5|\n",
            "| 11|Регион4|2022-03-10|120.0|          775|                 142.5|\n",
            "| 12|Регион4|2022-04-21|200.0|          733|                 142.5|\n",
            "+---+-------+----------+-----+-------------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Создание Spark сессии\n",
        "spark = SparkSession.builder.appName(\"window-functions\").getOrCreate()\n",
        "\n",
        "# Пример данных\n",
        "data = [(1, \"Регион1\", \"2022-01-01\", 100.0),\n",
        "        (2, \"Регион1\", \"2022-02-15\", 150.0),\n",
        "        (3, \"Регион2\", \"2022-01-10\", 120.0),\n",
        "        (4, \"Регион2\", \"2022-02-28\", 200.0),\n",
        "        (5, \"Регион2\", \"2022-03-31\", 100.0),\n",
        "        (6, \"Регион3\", \"2022-01-15\", 150.0),\n",
        "        (7, \"Регион3\", \"2022-02-11\", 120.0),\n",
        "        (8, \"Регион3\", \"2022-03-28\", 200.0),\n",
        "        (9, \"Регион4\", \"2022-01-03\", 100.0),\n",
        "        (10, \"Регион4\", \"2022-02-18\", 150.0),\n",
        "        (11, \"Регион4\", \"2022-03-10\", 120.0),\n",
        "        (12, \"Регион4\", \"2022-04-21\", 200.0)\n",
        "]\n",
        "\n",
        "# Создание DataFrame\n",
        "schema = [\"id\", \"region\", \"date\", \"sales\"]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Преобразование поля date в тип данных DateType\n",
        "df = df.withColumn(\"date\", F.to_date(\"date\"))\n",
        "\n",
        "# Вычисление разницы в днях между текущей датой и датой 90 дней назад\n",
        "df = df.withColumn(\"days_since_90\", F.datediff(F.current_date(), \"date\"))\n",
        "\n",
        "# Определение оконной спецификации\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 90)\n",
        "\n",
        "# Вычисление среднего объема продаж по регионам за последние 90 дней\n",
        "avg_sales_last_90_days = F.avg(\"sales\").over(window_spec)\n",
        "\n",
        "# Применение оконной функции и cast к DataFrame\n",
        "result_df = df.withColumn(\"avg_sales_last_90_days\", avg_sales_last_90_days.cast(DoubleType()))\n",
        "\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yPc6KpFFqqko"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFs2k8c3tY_5"
      },
      "source": [
        "### Задание 4\n",
        "Вам предоставлен набор данных с информацией о покупках в интернет-магазине. Необходимо написать пользовательскую функцию (udf), которая будет преобразовывать категорию товара в стоимость доставки. Для этого предположим, что стоимость доставки зависит от категории товара, следующим способом\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65HDnWGjwVVX"
      },
      "source": [
        "1. Инициализируем SparkSession: Создается сессия Spark, которая является точкой входа для работы с Spark API. Это необходимо для выполнения операций с данными.\n",
        "\n",
        "2. Создаем DataFrame: Создается DataFrame с двумя колонками: \"товар\" и \"категория\". DataFrame представляет собой распределенную коллекцию данных, организованную в колонки. В данном случае, DataFrame заполняется примерами данных о товарах и их категориях.\n",
        "\n",
        "3. Определяем пользовательской функции (UDF): Определяем функцию calculate_shipping_cost, которая принимает категорию товара в качестве аргумента и возвращаем стоимость доставки в зависимости от категории. Эта функция используется для расчета стоимости доставки для разных категорий товаров.\n",
        "\n",
        "4. Регистрируем UDF: Функция calculate_shipping_cost регистрируем как пользовательскую функцию (UDF) в Spark, что позволяет использовать ее в DataFrame операциях. UDF позволяет применять функции, написанные на Python, к данным в DataFrame.\n",
        "\n",
        "5. Применяем UDF к DataFrame: Применяется UDF к колонке \"категория\" DataFrame, создавая новую колонку \"стоимость_доставки\". Это делается с помощью метода withColumn, который добавляет новую колонку в DataFrame, результатом которой является вызов UDF для каждой строки в колонке \"категория\".\n",
        "\n",
        "6. Отображаем результатов: Выводим содержимое DataFrame с добавленной колонкой \"стоимость_доставки\" с помощью метода show(). Это позволяет увидеть результаты применения UDF к данным."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBLgPvbxrhRS",
        "outputId": "45d153a0-598d-4b91-b68c-947388295650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------+------------------+\n",
            "|            товар|  категория|стоимость_доставки|\n",
            "+-----------------+-----------+------------------+\n",
            "|         Смартфон|электроника|                50|\n",
            "|          Пуловер|     одежда|                30|\n",
            "|           Ролики|      спорт|                40|\n",
            "|Стиральная машина|электроника|                50|\n",
            "|         Эспандер|      спорт|                40|\n",
            "|Баскетбольный мяч|      спорт|                40|\n",
            "|      Холодильник|электроника|                50|\n",
            "|         Колготки|     одежда|                30|\n",
            "|         Футболка|     одежда|                30|\n",
            "+-----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Создаем сессию Spark\n",
        "spark = SparkSession.builder.appName(\"udf_example\").getOrCreate()\n",
        "\n",
        "# Пример данных\n",
        "data = [(\"Смартфон\", \"электроника\"),\n",
        "        (\"Пуловер\", \"одежда\"),\n",
        "        (\"Ролики\", \"спорт\"),\n",
        "        (\"Стиральная машина\", \"электроника\"),\n",
        "        (\"Эспандер\", \"спорт\"),\n",
        "        (\"Баскетбольный мяч\", \"спорт\"),\n",
        "        (\"Холодильник\", \"электроника\"),\n",
        "        (\"Колготки\", \"одежда\"),\n",
        "        (\"Футболка\", \"одежда\"),\n",
        "        ]\n",
        "df = spark.createDataFrame(data, [\"товар\", \"категория\"])\n",
        "\n",
        "# Стоимость доставки в зависимости от категории товара\n",
        "def calculate_shipping_cost(category):\n",
        "    if category == \"электроника\":\n",
        "        return 50\n",
        "    elif category == \"одежда\":\n",
        "        return 30\n",
        "    elif category == \"спорт\":\n",
        "        return 40\n",
        "    else:\n",
        "        return 20 # если категория не определена, предположим стандартную стоимость\n",
        "\n",
        "# Регистрируем пользовательскую функцию (udf)\n",
        "shipping_cost_udf = udf(calculate_shipping_cost, IntegerType())\n",
        "\n",
        "# Применяем удф к колонке \"категория\" и создаем новую колонку \"стоимость_доставки\"\n",
        "df = df.withColumn(\"стоимость_доставки\", shipping_cost_udf(df[\"категория\"]))\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dHPukbolvhNH"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVvBOgOYwk1F"
      },
      "source": [
        "### Задание 5\n",
        "Создаем новый столбец в DataFrame, используя пользовательскую функцию (udf). Функция должна принимать значение из одного столбца, выполнить какое-то простое вычисление и возвращать результат в новый столбец."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RELQuBXyxEfJ"
      },
      "source": [
        "1. Импортируем необходимые модули:\n",
        "    - SparkSession из pyspark.sql для создания сессии Spark.\n",
        "    - udf (User Defined Function) из pyspark.sql.functions для создания пользовательских функций.\n",
        "    - IntegerType из pyspark.sql.types для определения типа данных столбца в DataFrame.\n",
        "\n",
        "2. Создание сессии Spark:\n",
        "    - Используется SparkSession.builder.appName(\"udf_example\").getOrCreate() для создания или получения существующей сессии Spark с именем приложения \"udf_example\".\n",
        "\n",
        "3. Создание исходного DataFrame:\n",
        "    - Создаем DataFrame df с двумя столбцами: \"name\" и \"age\", содержащие имена и возраст людей соответственно.\n",
        "\n",
        "4. Определяем пользовательскую функцию (UDF):\n",
        "    - Определяем функцию double_age, которая принимает один аргумент (возраст) и возвращает его удвоенное значение.\n",
        "\n",
        "5. Регистрирум UDF:\n",
        "    - Функция double_age регистрируется как UDF с помощью udf(double_age, IntegerType()), где IntegerType() указывает, что возвращаемый тип данных функции будет целочисленным.\n",
        "\n",
        "6. Применением UDF к DataFrame:\n",
        "    - Применяем UDF double_age_udf к столбцу \"age\" исходного DataFrame df, создавая новый столбец \"double_age\", который содержит удвоенные значения возраста.\n",
        "\n",
        "Вывод результата: Выводим результат работы с DataFrame, включающий исходные данные и новый столбец \"double_age\" с удвоенными значениями возраста.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fdzV-Pfwmu_",
        "outputId": "20043dd0-6d08-490a-8bb9-659ca114e3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+----------+\n",
            "| name|age|double_age|\n",
            "+-----+---+----------+\n",
            "|Alice| 25|        50|\n",
            "|  Bob| 30|        60|\n",
            "|Chris| 35|        70|\n",
            "+-----+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Создаем Spark сессию\n",
        "spark = SparkSession.builder.appName(\"udf_example\").getOrCreate()\n",
        "\n",
        "# Пример исходного DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Chris\", 35)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Определяем пользовательскую функцию (udf)\n",
        "def double_age(age):\n",
        "\treturn age * 2\n",
        "\n",
        "# Регистрируем udf\n",
        "double_age_udf = udf(double_age, IntegerType())\n",
        "\n",
        "# Применяем udf к столбцу \"age\" и создаем новый столбец \"double_age\"\n",
        "df_with_double_age = df.withColumn(\"double_age\", double_age_udf(df[\"age\"]))\n",
        "\n",
        "# Выводим результат\n",
        "df_with_double_age.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "46ln0CoYwrIT"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoXC9TA6wwXT"
      },
      "source": [
        "### Задание 6\n",
        "Применяем пользовательскую функцию к данным в колонке DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq2rsK-8yD3Y"
      },
      "source": [
        "1. Импортируем необходимые модули: Сначала импортируем необходимые модули из библиотеки PySpark, включая SparkSession для создания сессии Spark, udf для создания пользовательских функций (UDF), и StringType для определения типа данных в UDF.\n",
        "\n",
        "2. Создаем сессии Spark: Создаем сессию Spark с именем приложения \"udf_example\". Это необходимо для работы с Spark DataFrame и выполнения операций с данными.\n",
        "\n",
        "3. Создаем DataFrame: Создается DataFrame с двумя столбцами: \"name\" и \"age\". DataFrame - это распределенная коллекция данных, которая может быть обработана с использованием Spark. В данном случае, DataFrame содержит имена и возраст людей.\n",
        "\n",
        "4. Определение пользовательской функции: Определяем функцию age_category, которая принимает возраст в качестве аргумента и возвращает строку \"Young\", если возраст меньше 30, и \"Old\" в противном случае. Эта функция предназначена для категоризации возраста.\n",
        "\n",
        "5. Создаем UDF: Создаем пользовательскую функция (UDF) с использованием функции age_category и типа данных StringType(). UDF позволяет применять пользовательские функции к столбцам DataFrame.\n",
        "\n",
        "6. Применяем UDF к DataFrame: Применяем созданную UDF к столбцу \"age\" DataFrame, создавая новый столбец \"age_category\", который содержит категории возраста для каждого человека.\n",
        "\n",
        "7. Отображаем результаты: Выводим содержимое DataFrame, включая новый столбец \"age_category\", на экран.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xG0i97vwuDj",
        "outputId": "c0711535-1035-4ff0-ffb5-904b6ee6710a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+------------+\n",
            "|   name|age|age_category|\n",
            "+-------+---+------------+\n",
            "|  Alice| 25|       Young|\n",
            "|    Bob| 30|         Old|\n",
            "|    Eve| 20|       Young|\n",
            "|Charlie| 35|         Old|\n",
            "+-------+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Создание Spark сессии\n",
        "spark = SparkSession.builder.appName(\"udf_example\").getOrCreate()\n",
        "\n",
        "# Создание DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Eve\", 20), (\"Charlie\", 35)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Создание пользовательской функции\n",
        "def age_category(age):\n",
        "    if age < 30:\n",
        "        return \"Young\"\n",
        "    else:\n",
        "        return \"Old\"\n",
        "\n",
        "age_category_udf = udf(age_category, StringType())\n",
        "\n",
        "# Применить пользовательскую функцию к колонке \"age\"\n",
        "df = df.withColumn(\"age_category\", age_category_udf(df.age))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AA0ok3qwyan9"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
