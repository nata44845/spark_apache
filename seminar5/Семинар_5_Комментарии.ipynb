{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-yDXhxrk9Zgs"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark >> None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtggnLn_-HUq"
   },
   "source": [
    "### Задача 1:\n",
    "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет среднее значение для каждого уникального ключа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZN8fEN-Gsdp"
   },
   "source": [
    "Данный код выполняет следующие действия:\n",
    "\n",
    "1. Импортирует необходимые модули из библиотеки PySpark: `SparkConf` и `SparkContext`.\n",
    "2. Создает объект `SparkConf`, который представляет конфигурацию для создания `SparkContext`.\n",
    "   - Метод `setAppName(\"Average By Key Example\")` устанавливает имя приложения в \"Average By Key Example\".\n",
    "   - Метод `setMaster(\"local[*]\")` устанавливает локальный режим выполнения Spark с использованием всех доступных ядер процессора.\n",
    "3. Создает объект `SparkContext` (sc) на основе конфигурации `SparkConf`.\n",
    "4. Создает RDD (Resilient Distributed Dataset) под названием `dataRDD` с помощью метода `parallelize`, передавая список пар ключ-значение.\n",
    "5. Преобразует RDD `dataRDD`, используя цепочку операций:\n",
    "   - Метод `mapValues` применяет функцию `lambda value: (value, 1)` к каждому значению RDD, превращая его в кортеж (значение, 1).\n",
    "   - Метод `reduceByKey` выполняет суммирование значений для каждого ключа, суммируя значения и соответствующие им счетчики.\n",
    "   - Метод `mapValues` применяет функцию `lambda sum_count: sum_count[0] / sum_count[1]` к каждой сумме и счетчику, вычисляя среднее значение для каждого ключа.\n",
    "6. Вызывает операцию `collect` на RDD `averageRDD` для получения всех результатов в локальном списке.\n",
    "7. Итерируется по результатам и выводит каждую пару ключ-значение.\n",
    "8. Вызывает метод `stop` у объекта `SparkContext` для остановки Spark контекста и освобождения ресурсов.\n",
    "\n",
    "Таким образом, данный код вычисляет среднее значение для каждого ключа в RDD `dataRDD` с использованием Spark и выводит результаты./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqUx9NNd8rk1",
    "outputId": "239eb71a-7697-4ce4-da06-3c9df5e1a53a"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 0.0 failed 1 times, most recent failure: Lost task 5.0 in stage 0.0 (TID 5) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m averageRDD \u001b[38;5;241m=\u001b[39m dataRDD \\\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28;01mlambda\u001b[39;00m value: (value, \u001b[38;5;241m1\u001b[39m)) \\\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: (x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m y[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m y[\u001b[38;5;241m1\u001b[39m])) \\\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28;01mlambda\u001b[39;00m sum_count: sum_count[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m sum_count[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Collect the data and print it\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43maverageRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key, value)\n",
      "File \u001b[1;32mc:\\Nata\\GeekBrains\\gb-git\\spark_apache\\venv_spark_apache\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Nata\\GeekBrains\\gb-git\\spark_apache\\venv_spark_apache\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Nata\\GeekBrains\\gb-git\\spark_apache\\venv_spark_apache\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 0.0 failed 1 times, most recent failure: Lost task 5.0 in stage 0.0 (TID 5) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"Average By Key Example\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "dataRDD = sc.parallelize([\n",
    "    (\"key1\", 10),\n",
    "    (\"key2\", 20),\n",
    "    (\"key1\", 30),\n",
    "    (\"key2\", 40),\n",
    "    (\"key1\", 50)\n",
    "])\n",
    "\n",
    "averageRDD = dataRDD \\\n",
    "    .mapValues(lambda value: (value, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda sum_count: sum_count[0] / sum_count[1])\n",
    "\n",
    "# Collect the data and print it\n",
    "result = averageRDD.collect()\n",
    "for key, value in result:\n",
    "    print(key, value)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alAKvc-c91l_"
   },
   "source": [
    "### Задача 2\n",
    "\n",
    "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет максимальное значение для каждого уникального ключа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeclI9ZdG6FT"
   },
   "source": [
    "Данный код использует PySpark для выполнения операции поиска максимального значения для каждого ключа в параллельно распределенном наборе данных (RDD).\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
    "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"MaxValueByKeyExample\".\n",
    "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
    "4. Создается список `data`, который содержит пары ключ-значение. Каждая пара представлена в виде кортежа.\n",
    "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
    "6. Выполняется операция `reduceByKey(max)` на RDD `rdd`. Операция `reduceByKey()` объединяет значения для каждого ключа и применяет функцию `max` для нахождения максимального значения.\n",
    "7. Результат операции `reduceByKey()` сохраняется в переменной `max_values`.\n",
    "8. Вызывается метод `collect()` на `max_values` для сбора всех результатов в драйвере программы.\n",
    "9. Затем происходит итерация по результатам (`results`), и для каждого результата выводится ключ и соответствующее максимальное значение с помощью оператора `print()`.\n",
    "10. Наконец, вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код выполняет операцию поиска максимального значения для каждого ключа в RDD с использованием Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FFxvzuW9RWx",
    "outputId": "b096de2c-c821-4d3c-a728-3243686edf6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key1 20\n",
      "key2 15\n",
      "key3 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"MaxValueByKeyExample\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "data = [(\"key1\", 10), (\"key2\", 5), (\"key1\", 20), (\"key2\", 15), (\"key3\", 8)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "max_values = rdd.reduceByKey(max)\n",
    "\n",
    "results = max_values.collect()\n",
    "for result in results:\n",
    "    print(result[0], result[1])\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1Cm5nVo-z6h"
   },
   "source": [
    "### Задача 3\n",
    "\n",
    "Напишите программу на Apache Spark, которая считает сумму всех чисел в заданном RDD (Resilient Distributed Dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPL9cHrJHG7H"
   },
   "source": [
    "Данный код использует PySpark для вычисления суммы элементов в распределенном наборе данных (RDD).\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
    "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"SumRDDExample\".\n",
    "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
    "4. Создается список `data`, который содержит числа.\n",
    "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
    "6. Выполняется операция `sum()` на RDD `rdd`. Операция `sum()` вычисляет сумму всех элементов в RDD.\n",
    "7. Результат операции `sum()` сохраняется в переменной `total_sum`.\n",
    "8. Выводится сообщение с помощью оператора `print()`, которое содержит сумму всех чисел.\n",
    "9. Вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код вычисляет сумму всех чисел в RDD с использованием Spark и выводит эту сумму на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJ-i7Ezb-TW6",
    "outputId": "5bc843c0-f21d-407d-a52e-63730a041bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма всех чисел: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"SumRDDExample\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "total_sum = rdd.sum()\n",
    "\n",
    "print(\"Сумма всех чисел:\", total_sum)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc--9guc-rL3"
   },
   "source": [
    "### Задача 4\n",
    "\n",
    "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет топ N ключей с наибольшим значением, где N - заданное число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt_Jo1HWHUWw"
   },
   "source": [
    "Данный код использует PySpark для нахождения N ключей с наибольшими значениями в параллельно распределенном наборе данных (RDD).\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
    "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"TopNKeysExample\".\n",
    "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
    "4. Создается список `data`, который содержит пары ключ-значение. Каждая пара представлена в виде кортежа.\n",
    "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
    "6. Устанавливается значение переменной `N`, которая определяет количество ключей с наибольшими значениями, которые будут выбраны.\n",
    "7. Выполняется операция `takeOrdered(N, key=lambda x: -x[1])` на RDD `rdd`. Операция `takeOrdered()` возвращает N элементов RDD, отсортированных в порядке возрастания на основе заданного ключа. В данном случае, ключом является второй элемент кортежа (значение), и сортировка осуществляется в порядке убывания, указав `-x[1]`.\n",
    "8. Результат операции `takeOrdered()` сохраняется в переменной `top_N_keys`.\n",
    "9. Затем происходит итерация по результатам (`top_N_keys`), и для каждой пары ключ-значение выводится ключ и значение с помощью оператора `print()`.\n",
    "10. Наконец, вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код находит N ключей с наибольшими значениями в RDD и выводит их на экран в порядке убывания значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwsctcXm-h91",
    "outputId": "daf249ed-3d17-44a4-92bf-5061eb6e25bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key3 20\n",
      "key4 15\n",
      "key1 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"TopNKeysExample\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "data = [(\"key1\", 10), (\"key2\", 5), (\"key3\", 20), (\"key4\", 15), (\"key5\", 8)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "N = 3\n",
    "\n",
    "top_N_keys = rdd.takeOrdered(N, key=lambda x: -x[1])\n",
    "\n",
    "for key, value in top_N_keys:\n",
    "    print(key, value)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgA14vPX_FSs"
   },
   "source": [
    "### Задача 5\n",
    "\n",
    "Напишите программу на Apache Spark, которая читает CSV-файл и выводит количество строк в файле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6Qcgg4QHkAq"
   },
   "source": [
    "Данный код использует PySpark для подсчета количества строк в CSV-файле с использованием Spark SQL и DataFrame API.\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируются необходимые модули: `SparkContext`, `SparkConf` из пакета `pyspark`, а также `SparkSession` из модуля `pyspark.sql`.\n",
    "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"CountCSVLinesExample\".\n",
    "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
    "4. Создается объект `SparkSession` (`spark`) с помощью метода `SparkSession.builder.getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
    "5. Устанавливается путь к CSV-файлу, который будет использоваться для подсчета количества строк. Путь указывается в переменной `csv_file_path`.\n",
    "6. С помощью метода `read.csv()` на объекте `spark` загружается CSV-файл. Методу передаются следующие аргументы: `csv_file_path` - путь к файлу, `header=True` - указывает, что первая строка файла содержит заголовки столбцов, `inferSchema=True` - указывает на автоматическое определение схемы данных.\n",
    "7. Результат чтения CSV-файла сохраняется в переменной `df` в виде DataFrame. DataFrame - это распределенная коллекция данных, организованная в именованные столбцы.\n",
    "8. Выполняется операция `count()` на объекте `df`. Операция `count()` возвращает количество строк в DataFrame.\n",
    "9. Результат операции `count()` сохраняется в переменной `line_count`.\n",
    "10. Выводится сообщение с помощью оператора `print()`, которое содержит количество строк в файле.\n",
    "11. Вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код загружает CSV-файл с использованием Spark SQL, подсчитывает количество строк в файле и выводит это число на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSdbw1op-ryL"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"CountCSVLinesExample\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "csv_file_path = \"path/to/your/csv/file.csv\"\n",
    "\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "line_count = df.count()\n",
    "\n",
    "print(\"Количество строк в файле:\", line_count)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VwGK-B6_KrX"
   },
   "source": [
    "### Задача 6\n",
    "\n",
    "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет сумму значений для каждого уникального ключа, но только для ключей, значение которых больше заданного порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPCmBOYuHzze"
   },
   "source": [
    "Данный код использует PySpark для фильтрации и суммирования значений, превышающих заданный порог, в распределенном наборе данных (RDD).\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
    "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"SumValuesAboveThreshold\".\n",
    "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
    "4. Создается список `data`, который содержит пары ключ-значение. Каждая пара представлена в виде кортежа.\n",
    "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
    "6. Устанавливается значение переменной `threshold`, которое определяет пороговое значение для фильтрации.\n",
    "7. Выполняется операция `filter(lambda x: x[1] > threshold)` на RDD `rdd`. Операция `filter()` фильтрует элементы RDD с использованием заданного условия, в данном случае, значения второго элемента кортежа должны быть больше порогового значения.\n",
    "8. Результат фильтрации сохраняется в переменной `filtered_rdd`.\n",
    "9. Выполняется операция `reduceByKey(lambda a, b: a + b)` на `filtered_rdd`. Операция `reduceByKey()` суммирует значения для каждого ключа в RDD. В данном случае, значения суммируются с использованием лямбда-функции `lambda a, b: a + b`.\n",
    "10. Результат операции `reduceByKey()` сохраняется в переменной `result`.\n",
    "11. Затем происходит итерация по результатам (`result`), и для каждой пары ключ-значение выводится ключ и значение с помощью оператора `print()`.\n",
    "12. Наконец, вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код фильтрует значения в RDD, оставляя только те, которые превышают заданный порог, а затем суммирует значения для каждого ключа и выводит результаты на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpSR4tWS_ZKH",
    "outputId": "f870f1dd-8d50-4b41-a30a-321abf72199a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key1: 30\n",
      "key2: 50\n",
      "key3: 40\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"SumValuesAboveThreshold\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "data = [(\"key1\", 10), (\"key2\", 20), (\"key1\", 30), (\"key3\", 40), (\"key2\", 50)]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "threshold = 25\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > threshold)\n",
    "\n",
    "result = filtered_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "for key, value in result.collect():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgbSwOTcABqu"
   },
   "source": [
    "### Задача 7\n",
    "\n",
    "У вас есть RDD со следующей структурой: (ключ, список значений). Напишите программу на Apache Spark, которая найдет среднюю длину списка значений для каждого ключа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0PzIHdkID1H"
   },
   "source": [
    "Данный код использует PySpark для нахождения средней длины списков значений для каждого ключа в распределенном наборе данных (RDD) с использованием Spark SQL и DataFrame API.\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
    "2. Создается объект `spark` с помощью метода `SparkSession.builder.appName(\"AverageListLength\").getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
    "3. Создается список `data`, который содержит пары ключ-список значений. Каждая пара представлена в виде кортежа.\n",
    "4. Создается RDD (`rdd`) из списка `data` с помощью метода `spark.sparkContext.parallelize(data)`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
    "5. Выполняется операция `mapValues(lambda x: len(x))` на RDD `rdd`. Операция `mapValues()` применяет заданную функцию к каждому значению в RDD, в данном случае, функция `lambda x: len(x)` возвращает длину списка значений.\n",
    "6. Результат операции `mapValues()` сохраняется в переменной `rdd_key_length`.\n",
    "7. Выполняется операция `combineByKey()` на `rdd_key_length`. Операция `combineByKey()` используется для агрегации значений по ключу. В данном случае, используются три функции:\n",
    "   - Первая функция `lambda x: (x, 1)` преобразует каждое значение в кортеж `(значение, 1)`.\n",
    "   - Вторая функция `lambda acc, x: (acc[0] + x, acc[1] + 1)` объединяет значения для каждого ключа, складывая длины и подсчитывая количество списков значений.\n",
    "   - Третья функция `lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])` объединяет результаты агрегации для разных партиций или ключей.\n",
    "8. Результат операции `combineByKey()` сохраняется в переменной `rdd_sum_count`.\n",
    "9. Выполняется операция `mapValues(lambda x: x[0] / x[1])` на `rdd_sum_count`. Операция `mapValues()` применяет заданную функцию к каждому значению в RDD, в данном случае, функция `lambda x: x[0] / x[1]` вычисляет среднюю длину путем деления суммы длин на количество списков значений.\n",
    "10. Результат операции `mapValues()` сохраняется в переменной `rdd_avg_length`.\n",
    "11. Выполняется операция `collect()` на `rdd_avg_length`, чтобы получить все результаты.\n",
    "12. Затем происходит итерация по результатам (`result`), и для каждого ключа и средней длины выводится сообщение с помощью оператора `print()`.\n",
    "13. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код находит среднюю длину списков значений для каждого ключа в RDD и выводит результаты на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1orWdv3c_Z4C",
    "outputId": "45c528e2-589b-4dfd-afae-c5cd6ec1527e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: key1, Average Length: 3.5\n",
      "Key: key2, Average Length: 2.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AverageListLength\").getOrCreate()\n",
    "\n",
    "data = [(\"key1\", [1, 2, 3]),\n",
    "        (\"key2\", [4, 5]),\n",
    "        (\"key1\", [6, 7, 8, 9]),\n",
    "        (\"key2\", [10, 11, 12])]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd_key_length = rdd.mapValues(lambda x: len(x))\n",
    "\n",
    "rdd_sum_count = rdd_key_length.combineByKey(\n",
    "    lambda x: (x, 1),\n",
    "    lambda acc, x: (acc[0] + x, acc[1] + 1),\n",
    "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    ")\n",
    "\n",
    "rdd_avg_length = rdd_sum_count.mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "result = rdd_avg_length.collect()\n",
    "for key, avg_length in result:\n",
    "    print(\"Key: {}, Average Length: {}\".format(key, avg_length))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoQRHVhPAMmW"
   },
   "source": [
    "### Задача 8\n",
    "\n",
    "У вас есть RDD, содержащий записи о посещениях веб-сайта со следующей структурой: (пользователь, дата, время). Напишите программу на Apache Spark, которая найдет количество уникальных пользователей для каждой даты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mABqh9qZIT-j"
   },
   "source": [
    "Данный код использует PySpark для подсчета уникальных пользователей для каждой даты в распределенном наборе данных (RDD) с использованием Spark SQL и DataFrame API.\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
    "2. Создается объект `spark` с помощью метода `SparkSession.builder.appName(\"UniqueUsersPerDate\").getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
    "3. Создается список `data`, который содержит записи о пользователях с указанием имени пользователя, даты и времени.\n",
    "4. Создается RDD (`rdd`) из списка `data` с помощью метода `spark.sparkContext.parallelize(data)`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
    "5. Выполняется операция `map(lambda x: (x[1], x[0]))` на RDD `rdd`. Операция `map()` применяет заданную функцию к каждому элементу в RDD, в данном случае, функция `lambda x: (x[1], x[0])` меняет местами значение даты и имени пользователя, чтобы ключом стала дата.\n",
    "6. Результат операции `map()` сохраняется в переменной `rdd_date_user`.\n",
    "7. Выполняется операция `distinct()` на `rdd_date_user`. Операция `distinct()` удаляет дублирующиеся элементы из RDD и оставляет только уникальные значения.\n",
    "8. Результат операции `distinct()` сохраняется в переменной `rdd_unique_users`.\n",
    "9. Выполняется операция `countByKey()` на `rdd_unique_users`. Операция `countByKey()` подсчитывает количество значений для каждого ключа в RDD и возвращает словарь, где ключами являются уникальные даты, а значениями - количество уникальных пользователей для каждой даты.\n",
    "10. Результат операции `countByKey()` сохраняется в переменной `result`.\n",
    "11. Затем происходит итерация по результатам (`result`), и для каждой даты и количества уникальных пользователей выводится сообщение с помощью оператора `print()`.\n",
    "12. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код находит уникальных пользователей для каждой даты в RDD и выводит результаты на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8Psq0oZAAsd",
    "outputId": "b197b28e-a91f-430a-b322-b6bdb057596e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2024-04-02, Unique Users: 2\n",
      "Date: 2024-04-03, Unique Users: 2\n",
      "Date: 2024-04-01, Unique Users: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UniqueUsersPerDate\").getOrCreate()\n",
    "\n",
    "data = [(\"user1\", \"2024-04-01\", \"10:00:00\"),\n",
    "        (\"user2\", \"2024-04-01\", \"11:30:00\"),\n",
    "        (\"user3\", \"2024-04-02\", \"09:45:00\"),\n",
    "        (\"user1\", \"2024-04-02\", \"14:20:00\"),\n",
    "        (\"user2\", \"2024-04-03\", \"16:30:00\"),\n",
    "        (\"user3\", \"2024-04-03\", \"18:15:00\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd_date_user = rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "rdd_unique_users = rdd_date_user.distinct()\n",
    "\n",
    "result = rdd_unique_users.countByKey()\n",
    "\n",
    "for date, count in result.items():\n",
    "    print(\"Date: {}, Unique Users: {}\".format(date, count))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYWcfsR5Ahmy"
   },
   "source": [
    "### Задача 9\n",
    "\n",
    "У вас есть два RDD с одинаковой структурой: (ключ, значение). Напишите программу на Apache Spark, которая выполнит объединение (join) этих RDD по ключу и найдет сумму значений для каждого уникального ключа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRFs6Y4qIjfM"
   },
   "source": [
    "Данный код использует PySpark для объединения двух распределенных наборов данных (RDD) по ключу и вычисления суммы значений для каждого ключа с использованием Spark SQL и DataFrame API.\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
    "2. Создается объект `spark` с помощью метода `SparkSession.builder.appName(\"RDDJoinAndSum\").getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
    "3. Создается список `data1`, который содержит пары ключ-значение.\n",
    "4. Создается RDD (`rdd1`) из списка `data1` с помощью метода `spark.sparkContext.parallelize(data1)`.\n",
    "5. Создается список `data2`, который содержит пары ключ-значение.\n",
    "6. Создается RDD (`rdd2`) из списка `data2` с помощью метода `spark.sparkContext.parallelize(data2)`.\n",
    "7. Выполняется операция `join(rdd2)` на RDD `rdd1`. Операция `join()` объединяет два RDD по ключу, в данном случае, по ключу \"key\". Результат операции сохраняется в переменной `rdd_join`.\n",
    "8. Выполняется операция `mapValues(lambda x: x[0] + x[1])` на `rdd_join`. Операция `mapValues()` применяет заданную функцию к каждому значению в RDD, в данном случае, функция `lambda x: x[0] + x[1]` складывает значения по ключу. Результат операции сохраняется в переменной `rdd_sum`.\n",
    "9. Выполняется операция `collect()` на `rdd_sum`, чтобы получить все результаты.\n",
    "10. Затем происходит итерация по результатам (`result`), и для каждого ключа и суммы значений выводится сообщение с помощью оператора `print()`.\n",
    "11. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код объединяет два RDD по ключу и вычисляет сумму значений для каждого ключа, выводя результаты на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHsMywoMAap-",
    "outputId": "393e9c7a-28bc-4938-ebae-8677d0097df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: key1, Sum: 5\n",
      "Key: key2, Sum: 7\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDJoinAndSum\").getOrCreate()\n",
    "\n",
    "data1 = [(\"key1\", 1),\n",
    "         (\"key2\", 2),\n",
    "         (\"key3\", 3)]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(data1)\n",
    "\n",
    "data2 = [(\"key1\", 4),\n",
    "         (\"key2\", 5),\n",
    "         (\"key4\", 6)]\n",
    "\n",
    "rdd2 = spark.sparkContext.parallelize(data2)\n",
    "\n",
    "rdd_join = rdd1.join(rdd2)\n",
    "\n",
    "rdd_sum = rdd_join.mapValues(lambda x: x[0] + x[1])\n",
    "\n",
    "result = rdd_sum.collect()\n",
    "for key, sum_value in result:\n",
    "    print(\"Key: {}, Sum: {}\".format(key, sum_value))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erFriTM-E-56"
   },
   "source": [
    "### Задача 10\n",
    "\n",
    "Подсчет количества уникальных слов в тексте с использованием DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW98kvt7Ix63"
   },
   "source": [
    "Данный код использует PySpark для подсчета количества уникальных слов в заданном тексте с использованием Spark SQL и DataFrame API.\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируются модули `SparkSession`, `explode`, `split` и `StringType` из соответствующих пакетов.\n",
    "2. Создается объект `spark` с помощью метода `SparkSession.builder`. Устанавливается имя приложения с помощью `appName(\"CountUniqueWords\")`.\n",
    "3. Указывается режим запуска приложения с помощью `master(\"local[*]\")`, где `\"local[*]\"` означает запуск приложения локально с использованием всех доступных ядер процессора.\n",
    "4. Вызывается метод `getOrCreate()` на объекте `spark` для создания или получения существующего сеанса Spark.\n",
    "5. Задается текст в виде списка строк `text`, который будет использован для создания DataFrame.\n",
    "6. Создается DataFrame `textDF` с помощью метода `spark.createDataFrame(text, StringType()).toDF(\"text\")`. Здесь каждая строка текста представляется как отдельная запись в DataFrame.\n",
    "7. Выполняется операция `split(\"text\", \" \")` на столбце `\"text\"` DataFrame `textDF`. Операция `split()` разделяет каждую строку текста на отдельные слова, используя пробел как разделитель.\n",
    "8. Выполняется операция `explode(...).alias(\"word\")` на результате операции `split()`. Операция `explode()` создает новую строку для каждого элемента в массиве слов. Результат операции сохраняется в DataFrame `wordsDF` с столбцом `\"word\"`.\n",
    "9. Выполняется операция `distinct()` на DataFrame `wordsDF`. Операция `distinct()` удаляет дублирующиеся строки и оставляет только уникальные слова.\n",
    "10. Результат операции `distinct()` сохраняется в DataFrame `uniqueWordsDF`.\n",
    "11. Выполняется операция `count()` на DataFrame `uniqueWordsDF` для подсчета количества уникальных слов.\n",
    "12. Результат подсчета сохраняется в переменной `uniqueWordCount`.\n",
    "13. Выводится сообщение с помощью оператора `print()` с указанием количества уникальных слов.\n",
    "14. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код создает DataFrame из заданного текста, разбивает текст на отдельные слова, находит уникальные слова и подсчитывает их количество, выводя результат на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__veW2EaEDmH",
    "outputId": "d6a63f9e-f994-438b-d0e9-191b44499804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word count: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CountUniqueWords\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "text = [\"Hello Spark\", \"Hello Scala\", \"Spark is awesome\"]\n",
    "textDF = spark.createDataFrame(text, StringType()).toDF(\"text\")\n",
    "wordsDF = textDF.select(explode(split(\"text\", \" \")).alias(\"word\"))\n",
    "uniqueWordsDF = wordsDF.distinct()\n",
    "uniqueWordCount = uniqueWordsDF.count()\n",
    "\n",
    "print(\"Unique word count:\", uniqueWordCount)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0a2WO_0KSqS"
   },
   "source": [
    "Данный код использует PySpark для создания объекта DataFrame из заданного списка словарей, фильтрации DataFrame по возрасту и вывода результата на экран.\n",
    "\n",
    "Вот пошаговое описание кода:\n",
    "\n",
    "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
    "2. Создается объект `spark` с помощью метода `SparkSession.builder`. Устанавливается имя приложения с помощью `appName(\"OOP Example\")`.\n",
    "3. Вызывается метод `getOrCreate()` на объекте `spark` для создания или получения существующего сеанса Spark.\n",
    "4. Задается список словарей `data`, представляющий данные, из которых будет создан DataFrame.\n",
    "5. Создается DataFrame `df` с помощью метода `spark.createDataFrame(data)`. DataFrame создается на основе заданного списка словарей, где каждый словарь представляет отдельную запись в DataFrame.\n",
    "6. Выполняется операция фильтрации на DataFrame `df` с помощью метода `filter(df.age > 30)`. Здесь фильтр применяется к столбцу `age` и оставляет только записи, где значение столбца `age` больше 30.\n",
    "7. Результат фильтрации сохраняется в DataFrame `df_filtered`.\n",
    "8. Вызывается метод `show()` на DataFrame `df_filtered`, чтобы вывести результат фильтрации на экран.\n",
    "9. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
    "\n",
    "Таким образом, данный код создает DataFrame из заданного списка словарей, фильтрует записи по возрасту (>30) и выводит результат фильтрации на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5o_byrTKTGu"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание объекта SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OOP Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 25},\n",
    "    {\"name\": \"Bob\", \"age\": 30},\n",
    "    {\"name\": \"Charlie\", \"age\": 35}\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df_filtered = df.filter(df.age > 30)\n",
    "df_filtered.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
